# -*- coding: utf-8 -*-
"""Boltzmann Machine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JxIEe_TAcz-utfLKTqjbo3foOtqDGD0s

#Boltzmann Machine

##Downloading the dataset

###ML-100K
"""

!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls

"""###ML-1M"""

!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip ml-1m.zip
!ls

"""##Importing the libraries"""

import numpy as np
import pandas as pd
#Model of torch to implement Neural Networks
import torch
import torch.nn as nn
#Implement parallel computataions
import torch.nn.parallel
#Optimiser(Loss)
import torch.optim as optim
# for stochastic Gradient Descent
import torch.utils.data
from torch.autograd import Variable

"""## Importing the dataset"""

# We won't be using this dataset.
#Arguments to read a Dat file:1.Path,2.) seperator: ::,header:there is no column name,engine:so that it could read in python,encoding: Latin-1 as it contains special characters
movies=pd.read_csv(r'C:\Users\KUNAL\Documents\Deep_Learning\Unsupervised_learning\Boltzmann_Machines\ml-1m\movies.dat',sep = '::',header=None,engine = 'python',encoding='latin-1')
users = pd.read_csv(r'C:\Users\KUNAL\Documents\Deep_Learning\Unsupervised_learning\Boltzmann_Machines\ml-1m\users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
ratings = pd.read_csv(r'C:\Users\KUNAL\Documents\Deep_Learning\Unsupervised_learning\Boltzmann_Machines\ml-1m\ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')

"""Movie Dataset:contains movie id,movie name with year,movie Type. 
Users:user id,gender,Age,code corresponds to user job,user pincodes.
Ratings:users 1,2nd ,movie id|no.,ratings(1-5),Timesteps:when the user rated the movie & thia we don't need so we remove it.
Examle of Ratings:1st user saw movie 1193 gave rating of 5 at a particular timestep
2.)
"""

"""## Preparing the training set and the test set :from ml-100k import it and convert the dataframe from pd to numpy array by np"""
#ml-100k-contains 1 lakh ratin's
#Traing set:80,000 ratings(base) & contains same columns as in ratings
#Test set:20,000 ratings(test) & here we only use u1 base & test split i.e only one train test split but initially it contains 5 train|test split
training_set=pd.read_csv(r"C:\Users\KUNAL\Documents\Deep_Learning\Unsupervised_learning\Boltzmann_Machines\ml-100k\u1.base",delimiter = '\t')
#Convert to numpy array the pd dataframe so as to use pytorch
training_set=np.array(training_set,dtype = 'int')#Dtype as int because we have numerical values
test_set=pd.read_csv(r"C:\Users\KUNAL\Documents\Deep_Learning\Unsupervised_learning\Boltzmann_Machines\ml-100k\u1.test",delimiter = '\t')
test_set=np.array(test_set,dtype = '\t')
#Vimp : For training & test set for same user we have different ratings of different movies

"""## Getting the number of users and movies: Why we are using this as we will make a 3d matrix where rows will be users,columns will be movies & each cell will be ratings as 0|1"""
"""Each cell of U,I i.e user  movie by 0,1:we will put 0 if the user didn't rate the movie & 1 if rated in train & test split of u1"""
# Take Total no. of Users & Movies by taking the Highest|Max of user no. & movie no. so that it will be applicable on U2,3,4,5 etc so as to k cross validation
nb_users=int(max(max(training_set[:,0],),max(test_set[:,0],)))# of u1
nb_movies=int(max(max(training_set[:,1],),max(test_set[:,1],)))

"""## Converting the data into an array with users in lines and movies in columns for both training & test set"""
# Here we will create lists of lists i.e 943 users and will have 1682 movies ratings
# lists of 943 users & list of 1682 movie ratings 
def convert(data):#Argument only 1 i.e data as we will going to apply fn to only 1 train & test set
      new_data=[];# list of lists
      for id_users in range(1,nb_users + 1):#To get rating of each user given to a movie
             id_movies=data[:,1][data[:,0]==id_users]#Taking all the movie id|no. from Training set then using the condition having all the movies id in column 0 and equaling to id_users to get the it
             id_ratings=data[:,2][data[:,0]==id_users]# To get the ratings of the first user which is in 3rd column in 
             #We will create a list of 1682 movies where the 1st user rated |not
             ratings=np.zeros(nb_movies)# Putting zeros to the ratings that were not rated by 1st user
             ratings[id_movies-1]=id_ratings#Getting ratings from id_movies and shifting its index from 0 to 1 and equal it to get real ratings
             new_data.append(list(ratings))#to get it in list
      return new_data
#Apply this fn to training & test set
training_set=convert(training_set)#contains a list of 943 users column wise and for each user we have the ratings of a movie watched by part. user i.e 1682 in columns
# 1st user rated 0 for movie 1 and 3 for movie 2 ... till 1682 movies if there
test_set=convert(test_set)      
"""## Converting the data into Torch tensors"""
#Pytorch Tensors:Tensor are muti-dimensional arrays that contains element of single data type from torch we include FloatTensors as ratings are in float
training_set = torch.FloatTensor(training_set)# in columns we have 1682 columns & 943 different users in rows 
test_set = torch.FloatTensor(test_set)

"""## Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked) from float 0f 0 to 5 in -1 & 1 for liked and not liked"""(RBM)
training_set[training_set==0]=-1#Not given any rating,all the zero values of training set by -1
training_set[training_set==1]=0#Not liked by users
training_set[training_set==2]=0
training_set[training_set>=3]=1#Liked by the users
test_set[test_set==0]=-1
test_set[test_set==1]=0
test_set[test_set==2]=0
test_set[test_set>=3]=1
"""## Creating the architecture of the Neural Network(RBM)"""(Probabilistic graphical model)
#self.a:means variable of object self of class RBM
# Class for modelling house,self driving class,rbm should be build.
#Architecture:hidden nodes,weights prob of VN given HN,bias
#Create 3 function inside class Rbm:1.initialize rbm parameters(weight,bias).2.sample_h:sample prob of Hn given Vn.3.sample_v:Sample prob of Vn given Hn
#Prob of Hn given Vn:Features(weights) identify,prob of Vn given Hn(Reconstruct the Vn & give prob to Vn which were not rated by the user)
#Sigmoid activation fn=weghts X x(visible node) + bias of Vn|Hn
class RBM():
    def __init__(self,nv,nh):#self is the object which initialize the parameters
        self.W=torch.randn(nh,nv)#prob of Hn given vn & vice versa
        self.a=torch.randn(1,nh)#create a 2d matrix by 1st argument batch,2nd argument with NH which make 2d tensor
        self.b=torch.randn(1,nv)#Bias for nv & nh:a & b
    def sample_h(self,x):#Sigmoid activation fn so as to calc likelihood of nh given nv,x is neurons in V given Prob h given V 
        wx=torch.mm(x,self.W.t())#product(mm) wx and taking its transpose
        activation=wx+self.a.expand_as(wx)#bias is applied to each line of mini batch that we create i.e to every batch by expand_as
        p_h_given_v=torch.sigmoid(activation)
        return p_h_given_v,torch.bernoulli(p_h_given_v)    #to provide sample prob oh hn(Gibbs sampling) by vector of ph_given_v
    def sample_v(self,y):#Sigmoid activation fn so as to calc likelihood of nv given nh,y is neurons in H given Prob V given H 
        wy=torch.mm(y,self.W)#not taken transpose as weight is matrix of pv given h 
        activation=wy+self.b.expand_as(wy)
        p_v_given_h=torch.sigmoid(activation)
        return p_v_given_h,torch.bernoulli(p_v_given_h)
    def train(self,v0, vk, ph0, phk):#Constrastive divergence:update weight,bias a & b
        #v0:input vector containing ratings of movies of 1 user,vk:after k samplings,ph0:prob at 1st iteration of HN=1 given Vn,Phk:prob Hn after k sampling,CD
        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()
        self.b+= torch.sum((v0-vk),0)# 0 2 keep in 2d i.e tensors
        self.a+=torch.sum((ph0-phk),0)
nv=len(training_set[0])   # can take nb_movies
nh=100# we have to choose ourself,no. of feature like oscar,director,type...
batch_size=100#take 100 users in first batch and so on 
rbm=RBM(nv,nh)
     
       
          
        
"""## Training the RBM"""
nb_epoch=10
for epoch in range(1,nb_epoch+1):
    train_loss=0#Calculate loss btw predicted and real ratings and to normalize we divide it by counter  
    s=0. # train_loss to be float we make it 0.
    for id_user in range(0,nb_users-batch_size,batch_size): # users:943 as the 3rd argument gives that 1st 100 users will be taken i.e 0-99 then so on
        vk=training_set[id_user:id_user:id_user+batch_size]#INPUT:give initial user 0-99
        v0=training_set[id_user:id_user+batch_size]#TARGET
        ph0,_ = rbm.sample_h(v0)#ph0-prob of Hn given 0|1 given Vn(real ratings)
        for k in range(10):#Contrastive divergence-making gibbs sampling
            _,hk=rbm.sample_h(vk)# Hn obtained at kth step and inverse as we need to call 2nd element _,hk
            _,vk=rbm.sample_v(hk)# to get 1st update on vn i.e hn ratings we get after 1st sampling activate green or red
            vk[v0<0] = v0[v0<0]
        phk,_=rbm.sample_h(vk)
        rbm.train(v0,vk,ph0,phk)
        train_loss+=torch.mean(torch.abs(v0[vo>=0]-vk[vo>=0]))#update train_loss to add the errors abs(-2)=2
        #loss : we have taken average distance as when we take rmse we get an error aroungd 0.46
        s+=1
    print("epoch:" +str(epoch)+ 'Loss:' +str(train_loss/s)) # + to concatenate strings epoch and str + for loss + for str
    
"""## Testing the RBM"""s
test_loss=0
s=0.
for id_user in range(nb_users):
    v= training_set[id_user:id_user+1]#v is the input on which we will test 
    vt=test_set[id_user:id_user+1]#vt is target
    if len(vt[vt>=0])>0:
        _,h=rbm.sample_h(v)
        _,v=rbm.sample_v(h)
        test_loss+=torch.mean(torch.abs(vt[vt>=0]-v[vt>=0]))
        s+=1.
print('test loss:' +str(test_loss/s))        
    

    
